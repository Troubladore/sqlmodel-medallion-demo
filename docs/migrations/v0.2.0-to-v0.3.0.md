# Migration Guide: v0.2.0 → v0.3.0 "Gold Layer Stabilization"

**Migration Type**: ⚠️ **CRITICAL** - Contains Breaking Changes  
**Estimated Time**: 30-60 minutes  
**Downtime Required**: ~5 minutes for schema recreation  
**Rollback Complexity**: Difficult (schema changes)  

---

## 🎯 Migration Overview

This migration transforms the platform from basic silver-gold processing to a production-ready dimensional analytics platform with:

- **Complete Star Schema**: Full dimensional modeling with 7/7 tables populated
- **Individual Customer Analytics**: 599 customer records vs 1 aggregated record
- **Strict Data Lineage**: Elimination of cross-layer fallback logic
- **Type Safety**: Spark 4.0.0 compatibility with explicit type casting
- **Fail-Fast Reliability**: 100% error detection and pipeline failure

---

## 🚨 CRITICAL Breaking Changes

### 1. Customer Metrics Schema Change (HIGH IMPACT)
**Before (v0.2.0)**: Single aggregated customer metrics record
**After (v0.3.0)**: Individual customer records (599 rows)

```sql
-- v0.2.0 Query Pattern (WILL BREAK)
SELECT total_revenue FROM gl_customer_metrics;
-- Returns: 1 row with platform total

-- v0.3.0 Query Pattern (REQUIRED)
SELECT customer_id, total_revenue, customer_tier 
FROM gl_customer_metrics 
WHERE customer_tier = 'Platinum'
ORDER BY total_revenue DESC;
-- Returns: Multiple rows, one per customer

-- Platform Total (if needed)
SELECT SUM(total_revenue) as platform_total_revenue
FROM gl_customer_metrics;
```

### 2. Error Handling Behavior Change (CRITICAL)
**Before (v0.2.0)**: Errors logged, pipeline continued
**After (v0.3.0)**: Any error fails entire pipeline immediately

**Required Action**: Update monitoring alerts to handle immediate failures instead of log-based detection.

### 3. Data Lineage Enforcement (CRITICAL)
**Before (v0.2.0)**: Gold transformations could fall back to bronze data
**After (v0.3.0)**: Gold MUST ONLY read from silver/gold layers

**Impact**: Any custom transformations with bronze fallback logic will fail.

---

## 🛠️ Pre-Migration Requirements

### System Requirements
- **Memory**: Minimum 4GB available (8GB+ recommended)
- **Storage**: Additional 500MB for new gold tables
- **Docker**: Ensure containers have adequate memory allocation

### Backup Procedures
```bash
# 1. Backup all databases
docker exec pagila pg_dump -U postgres -d silver_gold_dimensional > backup_silver_gold_v0.2.0.sql
docker exec pagila pg_dump -U postgres -d bronze_basic > backup_bronze_v0.2.0.sql

# 2. Export current metrics for validation
docker exec pagila psql -U postgres -d silver_gold_dimensional -c "COPY (SELECT * FROM pagila_gold.gl_customer_metrics) TO '/tmp/customer_metrics_v0.2.0.csv' CSV HEADER;"

# 3. Save container configurations
docker-compose config > docker-compose-v0.2.0-backup.yml
```

### Environment Validation
```bash
# Verify current version
grep -r "version.*0\.2\.0" src/

# Check available memory
docker stats --no-stream

# Validate current data
docker exec pagila psql -U postgres -d silver_gold_dimensional -c "SELECT COUNT(*) FROM pagila_gold.gl_customer_metrics;"
```

---

## 📋 Migration Steps

### Step 1: Code Updates (15 minutes)

#### 1.1 Update Silver-Gold Transformations
```bash
# Pull latest code with v0.3.0 changes
git fetch origin
git checkout v0.3.0  # Or merge latest changes

# Verify key files are updated
ls -la src/silver_gold_dimensional/transforms/silver_to_gold_dimensional.py
ls -la src/silver_gold_dimensional/CLAUDE.md
```

#### 1.2 Validate Type Safety Patterns
Review your custom transformations for type safety compliance:

```python
# ✅ CORRECT: Explicit type casting (v0.3.0)
.withColumn("customer_key", expr("uuid()").cast(StringType()))
.withColumn("is_active", lit(True).cast(BooleanType()))
.withColumn("amount", lit(0.0).cast(FloatType()))

# ❌ FORBIDDEN: Untyped literals (v0.2.0 - will fail)
.withColumn("customer_key", expr("uuid()"))  # Missing .cast()
.withColumn("is_active", lit(True))          # Missing .cast()
.withColumn("amount", lit(0.0))              # Missing .cast()
```

### Step 2: Infrastructure Preparation (5 minutes)

#### 2.1 Update Container Configuration
```bash
# Ensure adequate memory allocation in docker-compose.yml
# Spark driver should have at least 512MB

# Check current allocation
docker exec medallion-airflow-scheduler printenv | grep -i memory

# If needed, update docker-compose.yml and restart
docker-compose down
docker-compose up -d
```

#### 2.2 Validate Container Health
```bash
# Wait for all services to be healthy
docker-compose ps

# Check Airflow webserver accessibility
curl -f http://localhost:8080/health || echo "Airflow not ready"
```

### Step 3: Schema Migration (10 minutes)

#### 3.1 Backup Current Gold Schema
```bash
# Export current gold schema and data
docker exec pagila pg_dump -U postgres -d silver_gold_dimensional --schema=pagila_gold > pagila_gold_v0.2.0_backup.sql
```

#### 3.2 Recreate Gold Schema (DESTRUCTIVE - ensures clean state)
```sql
-- Connect to database
docker exec -it pagila psql -U postgres -d silver_gold_dimensional

-- Drop and recreate gold schema (removes old incompatible tables)
DROP SCHEMA IF EXISTS pagila_gold CASCADE;
CREATE SCHEMA pagila_gold;

-- Exit psql
\q
```

### Step 4: Pipeline Execution (15 minutes)

#### 4.1 Run Silver-Gold Pipeline
```bash
# Trigger complete pipeline rebuild
docker exec medallion-airflow-webserver airflow dags trigger silver_gold_dimensional_pipeline

# Monitor pipeline execution
watch -n 5 'docker exec medallion-airflow-webserver airflow tasks states-for-dag-run silver_gold_dimensional_pipeline $(date +%Y-%m-%d)'
```

#### 4.2 Monitor Progress
```bash
# Check task status (should show all success)
docker exec medallion-airflow-webserver airflow tasks states-for-dag-run silver_gold_dimensional_pipeline [RUN_ID]

# Monitor logs for errors
docker logs medallion-airflow-scheduler --tail 50 -f
```

### Step 5: Validation (10 minutes)

#### 5.1 Verify Gold Tables Population
```sql
-- Connect to database
docker exec -it pagila psql -U postgres -d silver_gold_dimensional

-- Check all gold tables are populated
SELECT 
    schemaname, 
    tablename, 
    n_tup_ins as row_count
FROM pg_stat_user_tables 
WHERE schemaname = 'pagila_gold'
ORDER BY tablename;

-- Expected results (approximate):
-- gl_dim_customer:     599 rows
-- gl_dim_date:      11,323 rows  
-- gl_dim_film:       1,000 rows
-- gl_dim_store:          2 rows
-- gl_dim_staff:          2 rows
-- gl_fact_rental:   16,049 rows
-- gl_customer_metrics: 599 rows
```

#### 5.2 Validate Customer Metrics Migration
```sql
-- Verify individual customer records exist
SELECT COUNT(*) as customer_count FROM pagila_gold.gl_customer_metrics;
-- Expected: 599 (not 1)

-- Check customer segmentation
SELECT customer_tier, COUNT(*) as customer_count 
FROM pagila_gold.gl_customer_metrics 
GROUP BY customer_tier 
ORDER BY customer_tier;

-- Verify revenue totals match
SELECT SUM(total_revenue) as total_platform_revenue 
FROM pagila_gold.gl_customer_metrics;
-- Should match previous platform total

-- Check top customers
SELECT customer_id, total_rentals, total_revenue, customer_tier
FROM pagila_gold.gl_customer_metrics 
ORDER BY total_revenue DESC 
LIMIT 5;
```

#### 5.3 Performance Validation
```bash
# Time a complete pipeline run
time docker exec medallion-airflow-webserver airflow dags trigger silver_gold_dimensional_pipeline

# Should complete in <60 seconds
```

---

## ✅ Post-Migration Validation Checklist

### Data Integrity Checks
- [ ] **Gold tables populated**: All 7 tables have expected row counts
- [ ] **Customer metrics**: 599 individual customer records (not 1 aggregate)
- [ ] **Revenue totals**: Sum of individual customer revenues matches previous total
- [ ] **Star schema**: All fact-dimension relationships intact
- [ ] **Data types**: No JDBC void type errors in logs

### Functional Validation  
- [ ] **Pipeline reliability**: 100% success rate on test runs
- [ ] **Query performance**: Customer analytics queries run <5 seconds
- [ ] **Error handling**: Test failures immediately stop pipeline
- [ ] **Data lineage**: No bronze fallback logic in gold transformations
- [ ] **Memory stability**: No OOM errors during processing

### Business Validation
- [ ] **Customer segmentation**: Platinum/Gold/Silver/Bronze tiers populated
- [ ] **BI queries**: Standard business intelligence queries work correctly
- [ ] **Dashboard compatibility**: Existing reports adapt to new schema
- [ ] **Analytics capabilities**: Individual customer insights available

---

## 🚨 Troubleshooting Common Issues

### Issue 1: Customer Metrics Table Empty
**Symptoms**: `gl_customer_metrics` has 0 rows or fails to create
**Causes**: Column ambiguity in fact table joins, missing customer_id field
**Resolution**:
```bash
# Check fact table schema
docker exec pagila psql -U postgres -d silver_gold_dimensional -c "\d pagila_gold.gl_fact_rental"

# Verify customer_id column exists
docker exec pagila psql -U postgres -d silver_gold_dimensional -c "SELECT COUNT(DISTINCT customer_id) FROM pagila_gold.gl_fact_rental;"

# If missing, re-run pipeline with updated transformation code
```

### Issue 2: JDBC Type Void Errors
**Symptoms**: Errors like "Can't get JDBC type for void"
**Causes**: Untyped `lit()` expressions in transformations
**Resolution**:
```python
# Find and fix untyped literals
# Replace: .withColumn("field", lit(None))
# With:    .withColumn("field", lit(None).cast(StringType()))

# Check all transformations for explicit type casting
grep -r "lit(" src/silver_gold_dimensional/transforms/
```

### Issue 3: Memory Issues
**Symptoms**: OOM errors, container restarts, slow processing
**Resolution**:
```bash
# Increase Spark memory allocation
# Edit docker-compose.yml:
# SPARK_CONF_spark_driver_memory: "1g"
# SPARK_CONF_spark_executor_memory: "1g"

# Restart containers
docker-compose down && docker-compose up -d
```

### Issue 4: Schema Conflicts
**Symptoms**: "Table already exists" or constraint violation errors
**Resolution**:
```sql
-- Nuclear option: completely recreate gold schema
DROP SCHEMA IF EXISTS pagila_gold CASCADE;
CREATE SCHEMA pagila_gold;
-- Then re-run pipeline
```

---

## 🔄 Rollback Procedures

### Quick Rollback (if migration fails early)
```bash
# 1. Stop current processes
docker-compose down

# 2. Restore previous version code
git checkout v0.2.0

# 3. Restore database from backup
docker-compose up -d
docker exec -i pagila psql -U postgres -d silver_gold_dimensional < backup_silver_gold_v0.2.0.sql

# 4. Verify rollback successful
docker exec pagila psql -U postgres -d silver_gold_dimensional -c "SELECT COUNT(*) FROM pagila_gold.gl_customer_metrics;"
# Should return 1 (aggregated record)
```

### Full Rollback (if issues discovered post-migration)
```bash
# 1. Document current v0.3.0 state for investigation
docker exec pagila pg_dump -U postgres -d silver_gold_dimensional > rollback_investigation_v0.3.0.sql

# 2. Complete rollback to v0.2.0
git checkout v0.2.0
docker-compose down
docker-compose up -d

# 3. Restore databases
docker exec -i pagila psql -U postgres -d silver_gold_dimensional < backup_silver_gold_v0.2.0.sql
docker exec -i pagila psql -U postgres -d bronze_basic < backup_bronze_v0.2.0.sql

# 4. Validate rollback
# Run validation queries from v0.2.0 documentation
```

---

## 📚 Updated Documentation

After successful migration, review updated documentation:

- **[EVOLUTION.md](../EVOLUTION.md)**: Platform evolution log with v0.3.0 achievements
- **[FEATURES.md](../FEATURES.md)**: Updated feature matrix and capabilities
- **[silver_gold_dimensional/CLAUDE.md](../../src/silver_gold_dimensional/CLAUDE.md)**: Implementation patterns and best practices
- **[ADR Records](../adr/)**: Architecture decisions for major changes

---

## 🎯 Success Criteria

Migration is successful when:

✅ **Pipeline Reliability**: 100% success rate on test runs  
✅ **Customer Analytics**: 599 individual customer metric records  
✅ **Performance**: <60 second end-to-end processing  
✅ **Data Quality**: Revenue totals match pre-migration values  
✅ **Error Handling**: Test failures immediately stop pipeline  
✅ **Memory Stability**: No OOM errors during normal operation  
✅ **Business Continuity**: All BI queries adapted to new schema  

---

**Migration Support**: For issues during migration, check troubleshooting section above or create detailed issue with migration logs.

**Next Steps**: After successful migration, review the new capabilities in the updated documentation and consider implementing advanced analytics features available in v0.3.0.

---

*Migration complete! Welcome to production-ready dimensional analytics with v0.3.0.*